{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(user=\"lvs215\",\n",
    "                        password=\"\",\n",
    "                        host=\"127.0.0.1\",\n",
    "                        port=\"12777\",\n",
    "                        database=\"aip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of papers that have at least one author assigned 71.21810595898887\n"
     ]
    }
   ],
   "source": [
    "# This cell outputs how many articles we have some author data on.\n",
    "with conn.cursor() as cursor:\n",
    "    query = \"\"\"\n",
    "SELECT count(id), (SELECT COUNT(DISTINCT(id))\n",
    "    FROM author_paper_pairs, publications \n",
    "    WHERE publications.id = author_paper_pairs.paper_id\n",
    ") AS number_papers_authors\n",
    "FROM publications\n",
    "\"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    row = cursor.fetchone()\n",
    "    print(\n",
    "        \"Amount of papers that have at least one author assigned {:.2f}\".format(\n",
    "            (row[1] / row[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of papers that have citation information 90.45\n"
     ]
    }
   ],
   "source": [
    "# This cell outputs how many articles we have some number of citation data on.\n",
    "with conn.cursor() as cursor:\n",
    "    query = \"\"\"\n",
    "SELECT count(id), (SELECT COUNT(DISTINCT(id))\n",
    "    FROM publications \n",
    "    WHERE publications.n_citations >= 0\n",
    ") AS number_articles_citation_info\n",
    "FROM publications\n",
    "\"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    row = cursor.fetchone()\n",
    "    print(\"Amount of papers that have citation information {:.2f}%\".format(\n",
    "        (row[1] / row[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check the amount of article entries in DBLP, Semantic Scholar, and AMiner.\n",
    "def wccount(filename):\n",
    "    out = subprocess.Popen(['wc', '-l', filename],\n",
    "                           stdout=subprocess.PIPE,\n",
    "                           stderr=subprocess.STDOUT\n",
    "                           ).communicate()[0]\n",
    "    return int(out.partition(b' ')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object iterload_file_lines_gzip at 0x2aab1496af90>\n",
      "Traceback (most recent call last):\n",
      "  File \"../util.py\", line 59, in iterload_file_lines_gzip\n",
      "    json_object = orjson.loads(line)\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from os.path import isfile\n",
    "\n",
    "from datetime import datetime\n",
    "from venue_mapper.venue_mapper import VenueMapper\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from util import iterload_file_lines, iterload_file_lines_gzip\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "def run():\n",
    "    total_dblp = 0\n",
    "    total_aminer = 0\n",
    "    total_semantic_scholar = 0\n",
    "    system_articles_dblp = 0\n",
    "    system_articles_aminer = 0\n",
    "    system_articles_semantic_scholar = 0\n",
    "    venue_mapper = VenueMapper()\n",
    "\n",
    "    file_locations = \"/var/scratch/lvs215/aip_data\"\n",
    "\n",
    "    # Create a list of all the files we want to parse. Skip the compressed sources if they are still lingering around\n",
    "    for path, subdirs, files in os.walk(file_locations):\n",
    "        for name in files:\n",
    "            if isfile(os.path.join(path, name)) and not name.endswith(\n",
    "                    (\"zip\", \"tar\")):\n",
    "                file_path = os.path.join(path, name)\n",
    "                if re.match(\".*dblp[\\w-]+\\.xml\", file_path):\n",
    "                    for event, element in etree.iterparse(file_path,\n",
    "                                                          load_dtd=True,\n",
    "                                                          dtd_validation=True):\n",
    "                        total_dblp += 1\n",
    "                        venue = element.find('booktitle')  # type: Optional[str]\n",
    "                        if venue is None and len(\n",
    "                                element.findall('journal')) > 0:\n",
    "                            venue = element.find('journal')\n",
    "\n",
    "                        if venue is not None and venue.text is not None:\n",
    "                            venue = str(venue.text)\n",
    "                        else:\n",
    "                            venue = None\n",
    "\n",
    "                        if venue is not None and venue_mapper.get_abbreviation(\n",
    "                                venue) is not None:\n",
    "                            system_articles_dblp += 1\n",
    "\n",
    "                elif \"s2-corpus\" in file_path:\n",
    "                    file_iterator_func = iterload_file_lines_gzip if file_path.endswith(\n",
    "                        \"gz\") else iterload_file_lines\n",
    "                    publication_iterator = file_iterator_func(file_path)\n",
    "                    for publication in publication_iterator:\n",
    "                        total_semantic_scholar += 1\n",
    "                        if publication is None:\n",
    "                            continue\n",
    "\n",
    "                        if \"venue\" not in publication:\n",
    "                            continue\n",
    "\n",
    "                        venue_string = str(publication['venue'])\n",
    "                        if len(venue_string) == 0:\n",
    "                            continue\n",
    "\n",
    "                        if venue_mapper.get_abbreviation(\n",
    "                                venue_string) is not None:\n",
    "                            system_articles_semantic_scholar += 1\n",
    "                elif \"aminer_papers\" in file_path:\n",
    "                    file_iterator_func = iterload_file_lines_gzip if file_path.endswith(\n",
    "                        \"gz\") else iterload_file_lines\n",
    "                    publication_iterator = file_iterator_func(file_path)\n",
    "                    for publication in publication_iterator:\n",
    "                        total_aminer += 1\n",
    "                        if publication is None:\n",
    "                            continue\n",
    "\n",
    "                        if 'venue' not in publication:\n",
    "                            continue\n",
    "\n",
    "                        venue_string = publication['venue']\n",
    "                        if isinstance(venue_string,\n",
    "                                      dict) and \"raw\" in venue_string:\n",
    "                            venue_string = venue_string[\"raw\"]\n",
    "\n",
    "                        if venue_mapper.get_abbreviation(\n",
    "                                venue_string) is not None:\n",
    "                            system_articles_aminer += 1\n",
    "\n",
    "    date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    with open(\"aip_content_per_source_{}.csv\".format(date_time), \"w\") as file1:\n",
    "        file1.write(\"dblp,{},{}\\n\".format(total_dblp, system_articles_dblp))\n",
    "        file1.write(\n",
    "            \"aminer,{},{}\\n\".format(total_aminer, system_articles_aminer))\n",
    "        file1.write(\"semantic_scholar,{},{}\\n\".format(total_semantic_scholar,\n",
    "                                                      system_articles_semantic_scholar))\n",
    "\n",
    "    print(\"DBLP: {}/{}: {:2f}\".format(total_dblp, system_articles_dblp,\n",
    "                                      total_dblp / system_articles_dblp))\n",
    "    print(\"Aminer: {}/{}: {:2f}\".format(total_aminer, system_articles_aminer,\n",
    "                                        total_aminer / system_articles_aminer))\n",
    "    print(\"Semantic Scholar: {}/{}: {:2f}\".format(total_semantic_scholar,\n",
    "                                                  system_articles_semantic_scholar,\n",
    "                                                  total_semantic_scholar / system_articles_semantic_scholar))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}